import { NextRequest, NextResponse } from 'next/server';
import OpenAI from 'openai';
import { getSystemPromptForContext } from '@/app/lib/services/promptService';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
});

export async function POST(request: NextRequest) {
  try {
    const { messages, context, isVoiceMode, pageContext } = await request.json();

    // D√©terminer le contexte (par d√©faut: chat)
    const contextType = pageContext || 'chat';

    // R√©cup√©rer le prompt syst√®me depuis la DB
    const promptConfig = await getSystemPromptForContext(contextType, context);

    let finalPrompt = promptConfig.systemPrompt;
    let maxTokens = 800;

    // ADAPTATION MODE VOCAL
    if (isVoiceMode) {
      finalPrompt += `

üé§ MODE VOCAL ACTIV√â :
R√àGLES STRICTES :
- R√©ponds en 2-3 phrases MAXIMUM
- Sois ultra-concis et direct
- Va √† l'essentiel, pas de d√©tails
- Une seule id√©e principale par r√©ponse
- Si liste n√©cessaire : 3 points MAX
- Ton amical mais efficace`;
      maxTokens = 150; // Forcer des r√©ponses courtes
    }

    const systemMessage = {
      role: 'system' as const,
      content: finalPrompt
    };

    const completion = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [systemMessage, ...messages],
      temperature: 0.2, // D√©terministe pour instructions bricolage pr√©cises
      max_tokens: maxTokens
    });

    return NextResponse.json({
      message: completion.choices[0].message.content,
      promptUsed: promptConfig.code // Pour debug
    });
  } catch (error) {
    console.error('Error in chat API:', error);
    return NextResponse.json(
      { error: 'Erreur lors de la g√©n√©ration de la r√©ponse' },
      { status: 500 }
    );
  }
}
